import os
import json
import requests
import numpy as np
from typing import List, Dict, Union, Optional
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import time
import tiktoken
import networkx as nx
from collections import defaultdict
import re

class GraphRAG:
    def __init__(self, docs: List[str], embed_model: str = "all-MiniLM-L6-v2"):
        self.docs = docs
        self.embedder = SentenceTransformer(embed_model)
        self.doc_vecs = self.embedder.encode(docs, convert_to_numpy=True)
        self.doc_map = {doc: f"doc_{i}" for i, doc in enumerate(docs)}  # æ·»åŠ doc_map
        self.graph = self._build_knowledge_graph()
        print(f"åˆå§‹åŒ–å›¾çŸ¥è¯†åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(docs)}")

    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        # ç®€å•çš„å®ä½“æå–ï¼šå‡è®¾å¤§å†™å¼€å¤´çš„è¯ç»„æ˜¯å®ä½“
        entities = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', text)
        return list(set(entities))

    def _build_knowledge_graph(self) -> nx.Graph:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        G = nx.Graph()
        entity_to_docs = defaultdict(set)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æå–å®ä½“å¹¶å»ºç«‹æ˜ å°„
        for doc_id, doc in enumerate(self.docs):
            entities = self._extract_entities(doc)
            for entity in entities:
                entity_to_docs[entity].add(self.doc_map[doc])  # ä½¿ç”¨doc_mapè·å–doc_id
                G.add_node(entity, type='entity')
                G.add_node(self.doc_map[doc], type='document', text=doc)  # ä½¿ç”¨doc_mapè·å–doc_id
                G.add_edge(entity, self.doc_map[doc], weight=1.0)

        # å»ºç«‹å®ä½“é—´çš„å…³ç³»ï¼ˆå¦‚æœå®ƒä»¬å‡ºç°åœ¨åŒä¸€æ–‡æ¡£ä¸­ï¼‰
        for entity1 in entity_to_docs:
            for entity2 in entity_to_docs:
                if entity1 < entity2:  # é¿å…é‡å¤è¾¹
                    common_docs = entity_to_docs[entity1] & entity_to_docs[entity2]
                    if common_docs:
                        G.add_edge(entity1, entity2, weight=len(common_docs))

        return G

    def _get_relevant_subgraph(self, question: str, k: int = 5) -> List[str]:
        """è·å–ä¸é—®é¢˜ç›¸å…³çš„å­å›¾å¹¶è¿”å›ç›¸å…³æ–‡æ¡£"""
        # ä»é—®é¢˜ä¸­æå–å®ä½“
        question_entities = self._extract_entities(question)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®ä½“ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢
        if not question_entities:
            return self._vector_search(question, k)
            
        # æ”¶é›†ä¸é—®é¢˜å®ä½“ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£èŠ‚ç‚¹
        relevant_docs = set()
        for entity in question_entities:
            if entity in self.graph:
                # ä½¿ç”¨ä¸ªæ€§åŒ–PageRankæ‰¾åˆ°æœ€ç›¸å…³çš„èŠ‚ç‚¹
                personalization = {node: 1.0 if node == entity else 0.0 
                                for node in self.graph.nodes()}
                ranks = nx.pagerank(self.graph, personalization=personalization)
                
                # è·å–æ–‡æ¡£èŠ‚ç‚¹
                doc_ranks = {node: rank for node, rank in ranks.items() 
                           if node.startswith('doc_')}
                
                # æ·»åŠ top-kæ–‡æ¡£
                top_docs = sorted(doc_ranks.items(), key=lambda x: x[1], reverse=True)[:k]
                relevant_docs.update(doc_id for doc_id, _ in top_docs)
        
        # å¦‚æœé€šè¿‡å›¾æ£€ç´¢æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„æ–‡æ¡£ï¼Œè¡¥å……å‘é‡æ£€ç´¢çš„ç»“æœ
        if len(relevant_docs) < k:
            vector_docs = self._vector_search(question, k - len(relevant_docs))
            # ä½¿ç”¨doc_mapå®‰å…¨åœ°è·å–doc_id
            relevant_docs.update(self.doc_map[d] for d in vector_docs if d in self.doc_map)
        
        # è¿”å›æ–‡æ¡£å†…å®¹
        return [self.graph.nodes[doc_id]['text'] 
                for doc_id in list(relevant_docs)[:k]]

    def _vector_search(self, question: str, k: int) -> List[str]:
        """å‘é‡æ£€ç´¢ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ"""
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        scores = cosine_similarity(q_vec, self.doc_vecs)[0]
        topk_idx = np.argsort(scores)[-k:][::-1]
        return [self.docs[i] for i in topk_idx]

    def rag_qa(self, question: str, k: int = 5) -> Dict:
        """ä¸NaiveRAGæ¥å£å…¼å®¹çš„æ£€ç´¢æ–¹æ³•"""
        start_time = time.time()
        
        # è·å–ç›¸å…³æ–‡æ¡£
        retrieved_docs = self._get_relevant_subgraph(question, k)
        
        # è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦åˆ†æ•°
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        doc_vecs = self.embedder.encode(retrieved_docs, convert_to_numpy=True)
        scores = cosine_similarity(q_vec, doc_vecs)[0]
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        retrieval_time = time.time() - start_time
        
        return {
            "docs": retrieved_docs,
            "doc_scores": [float(score) for score in scores],
            "metrics": {
                "retrieval_time": retrieval_time,
                "avg_similarity": float(np.mean(scores)),
                "max_similarity": float(np.max(scores)),
                "total_docs_searched": len(self.docs)
            }
        }


class GraphRAG_Improved(GraphRAG):
    def __init__(self, docs: List[str], embed_model: str = "all-MiniLM-L6-v2", max_pr_iter: int = 100):
        """
        æ”¹è¿›ç‰ˆGraphRAG
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            embed_model: ç¼–ç æ¨¡å‹åç§°
            max_pr_iter: PageRankæœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        # åœ¨è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ä¹‹å‰å…ˆåˆå§‹åŒ–spaCy
        self.max_pr_iter = max_pr_iter
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
            self.use_spacy = True
            print("âœ… æˆåŠŸåŠ è½½spaCyæ¨¡å‹")
        except Exception as e:
            print(f"âš ï¸ spaCyåŠ è½½å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼")
            self.use_spacy = False
            
        # ç°åœ¨è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–
        super().__init__(docs, embed_model)

    def _extract_entities(self, text: str) -> List[str]:
        """æ”¹è¿›çš„å®ä½“æå–ï¼Œä½¿ç”¨spaCy"""
        if self.use_spacy:
            doc = self.nlp(text)
            # æå–å‘½åå®ä½“å’Œåè¯çŸ­è¯­
            entities = set()
            # æ·»åŠ å‘½åå®ä½“
            entities.update(ent.text for ent in doc.ents)
            # æ·»åŠ é‡è¦çš„åè¯çŸ­è¯­
            entities.update(
                chunk.text for chunk in doc.noun_chunks 
                if len(chunk.text.split()) > 1  # åªä¿ç•™å¤šè¯çŸ­è¯­
                and not all(token.is_stop for token in chunk)  # æ’é™¤çº¯åœç”¨è¯
            )
            return list(entities)
        else:
            # å›é€€åˆ°æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
            # 1. å¤§å†™å¼€å¤´è¯ç»„
            upper_entities = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', text)
            # 2. é‡è¦çš„å°å†™è¯ç»„ï¼ˆè‡³å°‘ä¸¤ä¸ªè¯ï¼‰
            lower_entities = re.findall(r'\b[a-z]+\s+[a-z]+(?:\s+[a-z]+)*\b', text)
            all_entities = set(upper_entities + lower_entities)
            # è¿‡æ»¤æ‰å¸¸è§çš„æ— æ„ä¹‰è¯ç»„
            stop_patterns = {'is a', 'was a', 'has been', 'will be', 'can be'}
            return [e for e in all_entities if e.lower() not in stop_patterns]

    def _get_relevant_subgraph(self, question: str, k: int = 5) -> List[str]:
        """æ”¹è¿›çš„å­å›¾æ£€ç´¢æ–¹æ³•"""
        start_time = time.time()
        question_entities = self._extract_entities(question)
        print(f"ğŸ“Œ ä»é—®é¢˜ä¸­æå–çš„å®ä½“: {question_entities}")
        
        if not question_entities:
            print("âš ï¸ æœªæ‰¾åˆ°å®ä½“ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢")
            return self._vector_search(question, k)
            
        relevant_docs = set()
        graph_scores = defaultdict(float)  # å­˜å‚¨å›¾æ£€ç´¢å¾—åˆ†
        
        for entity in question_entities:
            if entity in self.graph:
                # é™åˆ¶PageRankæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé¿å…æ— å…³æ‰©æ•£
                personalization = {node: 1.0 if node == entity else 0.0 
                                for node in self.graph.nodes()}
                try:
                    ranks = nx.pagerank(self.graph, 
                                      personalization=personalization,
                                      max_iter=self.max_pr_iter)
                    
                    # è·å–æ–‡æ¡£èŠ‚ç‚¹
                    doc_ranks = {node: rank for node, rank in ranks.items() 
                               if node.startswith('doc_')}
                    
                    # ç´¯åŠ æ¯ä¸ªå®ä½“çš„PageRankå¾—åˆ†
                    for doc_id, rank in doc_ranks.items():
                        graph_scores[doc_id] += rank
                    
                    print(f"âœ… å®ä½“ '{entity}' æˆåŠŸæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"âš ï¸ PageRankè®¡ç®—å¤±è´¥: {e}")
                    continue
        
        # å¦‚æœå›¾æ£€ç´¢æ‰¾åˆ°äº†æ–‡æ¡£
        if graph_scores:
            # è·å–åˆæ­¥çš„top-k*2æ–‡æ¡£
            candidate_docs = sorted(graph_scores.items(), 
                                 key=lambda x: x[1], 
                                 reverse=True)[:k*2]
            
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é‡æ’åº
            texts = [self.graph.nodes[doc_id]["text"] for doc_id, _ in candidate_docs]
            if texts:  # ç¡®ä¿æœ‰æ–‡æ¡£å†ç¼–ç 
                doc_vecs = self.embedder.encode(texts, convert_to_numpy=True)
                q_vec = self.embedder.encode([question], convert_to_numpy=True)
                sims = cosine_similarity(q_vec, doc_vecs)[0]
                
                # ç»¼åˆè€ƒè™‘å›¾å¾—åˆ†å’Œå‘é‡ç›¸ä¼¼åº¦
                final_scores = [(doc_id, 0.5 * graph_score + 0.5 * sims[i])
                              for i, (doc_id, graph_score) in enumerate(candidate_docs)]
                
                # å–top-k
                top_docs = sorted(final_scores, key=lambda x: x[1], reverse=True)[:k]
                relevant_docs.update(doc_id for doc_id, _ in top_docs)
                
                print(f"ğŸ“Š å›¾æ£€ç´¢æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # å¦‚æœéœ€è¦è¡¥å……æ–‡æ¡£
        if len(relevant_docs) < k:
            needed = k - len(relevant_docs)
            print(f"âš ï¸ å›¾æ£€ç´¢æ–‡æ¡£ä¸è¶³ï¼Œéœ€è¦è¡¥å…… {needed} ä¸ªæ–‡æ¡£")
            
            # è·å–å‘é‡æ£€ç´¢ç»“æœ
            vector_docs = self._vector_search(question, needed * 2)  # å¤šæ£€ç´¢ä¸€äº›å€™é€‰
            
            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
            vector_vecs = self.embedder.encode(vector_docs, convert_to_numpy=True)
            q_vec = self.embedder.encode([question], convert_to_numpy=True)
            sims = cosine_similarity(q_vec, vector_vecs)[0]
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿‡æ»¤å·²æœ‰æ–‡æ¡£
            scored_docs = [(doc, sim) for doc, sim in zip(vector_docs, sims)
                          if self.doc_map[doc] not in relevant_docs]
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            # æ·»åŠ top-neededæ–‡æ¡£
            fallback_docs = {self.doc_map[doc] for doc, _ in scored_docs[:needed]}
            relevant_docs.update(fallback_docs)
            print(f"ğŸ“Š å‘é‡æ£€ç´¢è¡¥å……äº† {len(fallback_docs)} ä¸ªæ–‡æ¡£")
        
        retrieval_time = time.time() - start_time
        print(f"â±ï¸ æ€»æ£€ç´¢æ—¶é—´: {retrieval_time:.2f}ç§’")
        
        return [self.graph.nodes[doc_id]['text'] 
                for doc_id in list(relevant_docs)[:k]]

    def _vector_search(self, question: str, k: int) -> List[str]:
        """æ”¹è¿›çš„å‘é‡æ£€ç´¢ï¼Œå¢åŠ è°ƒè¯•ä¿¡æ¯"""
        start_time = time.time()
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        scores = cosine_similarity(q_vec, self.doc_vecs)[0]
        topk_idx = np.argsort(scores)[-k:][::-1]
        topk_scores = scores[topk_idx]
        
        print(f"ğŸ“Š å‘é‡æ£€ç´¢å¾—åˆ†èŒƒå›´: {topk_scores.min():.3f} - {topk_scores.max():.3f}")
        retrieval_time = time.time() - start_time
        print(f"â±ï¸ å‘é‡æ£€ç´¢æ—¶é—´: {retrieval_time:.2f}ç§’")
        
        return [self.docs[i] for i in topk_idx]

"""
DeepSieve RAG-only pipeline

This script implements a modular RAG pipeline with the following features:
- LLM-based query decomposition (optional)
- Per-subquestion routing to local/global knowledge sources
- Light-weight vector RAG with top-k retrieval
- Structured LLM prompting for reasoning + JSON parsing
- Reflection mechanism to reroute or rephrase failed queries
- Final answer fusion based on reasoning trace
- Comprehensive logging of retrieval, token usage, and accuracy metrics

Usage:
    Configure flags like `decompose`, `use_routing`, `use_reflection` to ablate components.
    Example:
        python script.py --decompose True --use_routing True --use_reflection True

Output:
    Each query's full trace is saved in outputs/{dataset_name}/query_{i}_results.jsonl
    Aggregated metrics saved in overall_results.json

Compatible datasets:
    - hotpot_qa
    - 2wikimultihopqa
    - musique
"""


import os
import json
import requests
import numpy as np
from typing import List, Dict, Union, Optional
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import time
import tiktoken
import networkx as nx
from collections import defaultdict
import re
from rag.naive_rag import NaiveRAG

# LLM è°ƒç”¨

def call_openai_chat(prompt: str, api_key: str, model: str, base_url: str, max_retries: int = 3) -> str:
    """è°ƒç”¨ OpenAI Chat API çš„å‡½æ•°ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶

    Args:
        prompt: æç¤ºæ–‡æœ¬
        api_key: APIå¯†é’¥
        model: æ¨¡å‹åç§°
        base_url: APIåŸºç¡€URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        str: APIå“åº”çš„æ–‡æœ¬å†…å®¹
    """
    url = f"{base_url}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
    }
    
    session = requests.Session()
    retry_strategy = requests.adapters.Retry(
        total=max_retries,
        backoff_factor=2,  # å¢åŠ é€€é¿æ—¶é—´
        status_forcelist=[500, 502, 503, 504, 408, 429],  # æ·»åŠ è¶…æ—¶å’Œé™æµçŠ¶æ€ç 
        allowed_methods=["POST"]
    )
    adapter = requests.adapters.HTTPAdapter(max_retries=retry_strategy, pool_maxsize=100)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    try:
        response = session.post(url, headers=headers, json=payload, timeout=60)  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
        response.raise_for_status()  # æ£€æŸ¥å“åº”çŠ¶æ€
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        print(f"ğŸ”´ APIè¯·æ±‚é”™è¯¯: {str(e)}")
        if isinstance(e, (requests.exceptions.ChunkedEncodingError, requests.exceptions.ReadTimeout)):
            print("æ£€æµ‹åˆ°è¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯•...")
            # å¯¹äºè¿æ¥é”™è¯¯ï¼Œæˆ‘ä»¬ç‰¹åˆ«å¤„ç†
            for i in range(max_retries):
                try:
                    print(f"é‡è¯• #{i+1}...")
                    time.sleep(2 ** i)  # æŒ‡æ•°é€€é¿
                    response = session.post(url, headers=headers, json=payload, timeout=60)
                    response.raise_for_status()
                    return response.json()["choices"][0]["message"]["content"]
                except requests.exceptions.RequestException as retry_e:
                    print(f"é‡è¯• #{i+1} å¤±è´¥: {str(retry_e)}")
                    if i == max_retries - 1:  # å¦‚æœæ˜¯æœ€åä¸€æ¬¡é‡è¯•
                        print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                        return ""
    except Exception as e:
        print(f"ğŸ”´ å…¶ä»–é”™è¯¯: {str(e)}")
        return ""
    finally:
        session.close()


def plan_subqueries_with_llm(query: str, api_key: str, model: str, base_url: str) -> dict:
    prompt = f"""You are a reasoning planner. Your task is to decompose a multi-hop question into a sequence of dependent sub-questions.
For each sub-question, you should:
1. Identify any variables that need to be filled from previous sub-questions' answers
2. Specify the dependency relationship between sub-questions
3. Use consistent variable names in square brackets (e.g. [birthplace]) to show dependencies

Question: {query}

Please output in JSON format as follows:
{{
    "subqueries": [
        {{
            "id": "q1",
            "query": "First sub-question without dependencies",
            "depends_on": [],
            "variables": []
        }},
        {{
            "id": "q2",
            "query": "Second sub-question that may contain [variable_from_q1]",
            "depends_on": ["q1"],
            "variables": [
                {{
                    "name": "variable_from_q1",
                    "source_query": "q1"
                }}
            ]
        }},
        ...
    ]
}}
Only output valid JSON. Do not add any explanation or markdown code block markers."""

    response = call_openai_chat(prompt, api_key, model, base_url)
    try:
        # æ¸…ç†å“åº”ä¸­å¯èƒ½å­˜åœ¨çš„markdownä»£ç å—æ ‡è®°
        cleaned_response = str(response).strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        
        result = json.loads(cleaned_response)
        if "subqueries" not in result:
            print("âš ï¸ å“åº”ä¸­ç¼ºå°‘subquerieså­—æ®µ:")
            print(result)
            return {"subqueries": []}
        return result
    except json.JSONDecodeError as e:
        print("âš ï¸ Failed to parse JSON from LLM response:")
        print(response)
        print(f"Error: {str(e)}")
        return {"subqueries": []}


def substitute_variables(query: str, variable_values: dict) -> str:
    """
    æ›¿æ¢æŸ¥è¯¢ä¸­çš„å˜é‡ä¸ºå…¶å®é™…å€¼
    ä¾‹å¦‚ï¼šå°† "What country is [birthplace] in?" ä¸­çš„ [birthplace] æ›¿æ¢ä¸ºå®é™…å€¼
    """
    result = query
    for var_name, value in variable_values.items():
        result = result.replace(f"[{var_name}]", value)
    return result


def route_query_with_llm(query: str, local_profile: str, global_profile: str,
                         api_key: str, model: str, base_url: str, fail_history: str) -> str:
    """è·¯ç”±æŸ¥è¯¢åˆ°åˆé€‚çš„çŸ¥è¯†åº“

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        local_profile: æœ¬åœ°çŸ¥è¯†åº“æè¿°
        global_profile: å…¨å±€çŸ¥è¯†åº“æè¿°
        api_key: APIå¯†é’¥
        model: æ¨¡å‹åç§°
        base_url: APIåŸºç¡€URL
        fail_history: å¤±è´¥å†å²

    Returns:
        str: è·¯ç”±ç»“æœ ("local" æˆ– "global")
    """
    prompt = f"""You are a routing assistant. Your task is to decide whether a query should be answered using LOCAL knowledge or GLOBAL knowledge.

LOCAL PROFILE:
{local_profile}

GLOBAL PROFILE:
{global_profile}

QUERY:
{query}

{fail_history}

Please output only one word: \"local\" or \"global\" based on which profile is more relevant to the query.
Do not add any explanation or extra words."""

    try:
        response = call_openai_chat(prompt, api_key, model, base_url)
        if not response:  # å¦‚æœå“åº”ä¸ºç©º
            print("âš ï¸ è·¯ç”±å“åº”ä¸ºç©ºï¼Œé»˜è®¤ä½¿ç”¨localè·¯ç”±")
            return "local"
        
        route = response.strip().lower()
        if route not in {"local", "global"}:
            print(f"âš ï¸ æ„å¤–çš„è·¯ç”±è¾“å‡º: {route}ï¼Œé»˜è®¤ä½¿ç”¨localè·¯ç”±")
            return "local"
        return route
    except Exception as e:
        print(f"âš ï¸ è·¯ç”±è¿‡ç¨‹å‡ºé”™: {str(e)}ï¼Œé»˜è®¤ä½¿ç”¨localè·¯ç”±")
        return "local"



class GraphRAG:
    def __init__(self, docs: List[str], embed_model: str = "all-MiniLM-L6-v2"):
        self.docs = docs
        self.embedder = SentenceTransformer(embed_model)
        self.doc_vecs = self.embedder.encode(docs, convert_to_numpy=True)
        self.doc_map = {doc: f"doc_{i}" for i, doc in enumerate(docs)}  # æ·»åŠ doc_map
        self.graph = self._build_knowledge_graph()
        print(f"åˆå§‹åŒ–å›¾çŸ¥è¯†åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(docs)}")

    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        # ç®€å•çš„å®ä½“æå–ï¼šå‡è®¾å¤§å†™å¼€å¤´çš„è¯ç»„æ˜¯å®ä½“
        entities = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', text)
        return list(set(entities))

    def _build_knowledge_graph(self) -> nx.Graph:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        G = nx.Graph()
        entity_to_docs = defaultdict(set)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æå–å®ä½“å¹¶å»ºç«‹æ˜ å°„
        for doc_id, doc in enumerate(self.docs):
            entities = self._extract_entities(doc)
            for entity in entities:
                entity_to_docs[entity].add(self.doc_map[doc])  # ä½¿ç”¨doc_mapè·å–doc_id
                G.add_node(entity, type='entity')
                G.add_node(self.doc_map[doc], type='document', text=doc)  # ä½¿ç”¨doc_mapè·å–doc_id
                G.add_edge(entity, self.doc_map[doc], weight=1.0)

        # å»ºç«‹å®ä½“é—´çš„å…³ç³»ï¼ˆå¦‚æœå®ƒä»¬å‡ºç°åœ¨åŒä¸€æ–‡æ¡£ä¸­ï¼‰
        for entity1 in entity_to_docs:
            for entity2 in entity_to_docs:
                if entity1 < entity2:  # é¿å…é‡å¤è¾¹
                    common_docs = entity_to_docs[entity1] & entity_to_docs[entity2]
                    if common_docs:
                        G.add_edge(entity1, entity2, weight=len(common_docs))

        return G

    def _get_relevant_subgraph(self, question: str, k: int = 5) -> List[str]:
        """è·å–ä¸é—®é¢˜ç›¸å…³çš„å­å›¾å¹¶è¿”å›ç›¸å…³æ–‡æ¡£"""
        # ä»é—®é¢˜ä¸­æå–å®ä½“
        question_entities = self._extract_entities(question)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®ä½“ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢
        if not question_entities:
            return self._vector_search(question, k)
            
        # æ”¶é›†ä¸é—®é¢˜å®ä½“ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£èŠ‚ç‚¹
        relevant_docs = set()
        for entity in question_entities:
            if entity in self.graph:
                # ä½¿ç”¨ä¸ªæ€§åŒ–PageRankæ‰¾åˆ°æœ€ç›¸å…³çš„èŠ‚ç‚¹
                personalization = {node: 1.0 if node == entity else 0.0 
                                for node in self.graph.nodes()}
                ranks = nx.pagerank(self.graph, personalization=personalization)
                
                # è·å–æ–‡æ¡£èŠ‚ç‚¹
                doc_ranks = {node: rank for node, rank in ranks.items() 
                           if node.startswith('doc_')}
                
                # æ·»åŠ top-kæ–‡æ¡£
                top_docs = sorted(doc_ranks.items(), key=lambda x: x[1], reverse=True)[:k]
                relevant_docs.update(doc_id for doc_id, _ in top_docs)
        
        # å¦‚æœé€šè¿‡å›¾æ£€ç´¢æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„æ–‡æ¡£ï¼Œè¡¥å……å‘é‡æ£€ç´¢çš„ç»“æœ
        if len(relevant_docs) < k:
            vector_docs = self._vector_search(question, k - len(relevant_docs))
            # ä½¿ç”¨doc_mapå®‰å…¨åœ°è·å–doc_id
            relevant_docs.update(self.doc_map[d] for d in vector_docs if d in self.doc_map)
        
        # è¿”å›æ–‡æ¡£å†…å®¹
        return [self.graph.nodes[doc_id]['text'] 
                for doc_id in list(relevant_docs)[:k]]

    def _vector_search(self, question: str, k: int) -> List[str]:
        """å‘é‡æ£€ç´¢ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ"""
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        scores = cosine_similarity(q_vec, self.doc_vecs)[0]
        topk_idx = np.argsort(scores)[-k:][::-1]
        return [self.docs[i] for i in topk_idx]

    def rag_qa(self, question: str, k: int = 5) -> Dict:
        """ä¸NaiveRAGæ¥å£å…¼å®¹çš„æ£€ç´¢æ–¹æ³•"""
        start_time = time.time()
        
        # è·å–ç›¸å…³æ–‡æ¡£
        retrieved_docs = self._get_relevant_subgraph(question, k)
        
        # è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦åˆ†æ•°
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        doc_vecs = self.embedder.encode(retrieved_docs, convert_to_numpy=True)
        scores = cosine_similarity(q_vec, doc_vecs)[0]
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        retrieval_time = time.time() - start_time
        
        return {
            "docs": retrieved_docs,
            "doc_scores": [float(score) for score in scores],
            "metrics": {
                "retrieval_time": retrieval_time,
                "avg_similarity": float(np.mean(scores)),
                "max_similarity": float(np.max(scores)),
                "total_docs_searched": len(self.docs)
            }
        }


class GraphRAG_Improved(GraphRAG):
    def __init__(self, docs: List[str], embed_model: str = "all-MiniLM-L6-v2", max_pr_iter: int = 100):
        """
        æ”¹è¿›ç‰ˆGraphRAG
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            embed_model: ç¼–ç æ¨¡å‹åç§°
            max_pr_iter: PageRankæœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        # åœ¨è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ä¹‹å‰å…ˆåˆå§‹åŒ–spaCy
        self.max_pr_iter = max_pr_iter
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
            self.use_spacy = True
            print("âœ… æˆåŠŸåŠ è½½spaCyæ¨¡å‹")
        except Exception as e:
            print(f"âš ï¸ spaCyåŠ è½½å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼")
            self.use_spacy = False
            
        # ç°åœ¨è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–
        super().__init__(docs, embed_model)

    def _extract_entities(self, text: str) -> List[str]:
        """æ”¹è¿›çš„å®ä½“æå–ï¼Œä½¿ç”¨spaCy"""
        if self.use_spacy:
            doc = self.nlp(text)
            # æå–å‘½åå®ä½“å’Œåè¯çŸ­è¯­
            entities = set()
            # æ·»åŠ å‘½åå®ä½“
            entities.update(ent.text for ent in doc.ents)
            # æ·»åŠ é‡è¦çš„åè¯çŸ­è¯­
            entities.update(
                chunk.text for chunk in doc.noun_chunks 
                if len(chunk.text.split()) > 1  # åªä¿ç•™å¤šè¯çŸ­è¯­
                and not all(token.is_stop for token in chunk)  # æ’é™¤çº¯åœç”¨è¯
            )
            return list(entities)
        else:
            # å›é€€åˆ°æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
            # 1. å¤§å†™å¼€å¤´è¯ç»„
            upper_entities = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', text)
            # 2. é‡è¦çš„å°å†™è¯ç»„ï¼ˆè‡³å°‘ä¸¤ä¸ªè¯ï¼‰
            lower_entities = re.findall(r'\b[a-z]+\s+[a-z]+(?:\s+[a-z]+)*\b', text)
            all_entities = set(upper_entities + lower_entities)
            # è¿‡æ»¤æ‰å¸¸è§çš„æ— æ„ä¹‰è¯ç»„
            stop_patterns = {'is a', 'was a', 'has been', 'will be', 'can be'}
            return [e for e in all_entities if e.lower() not in stop_patterns]

    def _get_relevant_subgraph(self, question: str, k: int = 5) -> List[str]:
        """æ”¹è¿›çš„å­å›¾æ£€ç´¢æ–¹æ³•"""
        start_time = time.time()
        question_entities = self._extract_entities(question)
        print(f"ğŸ“Œ ä»é—®é¢˜ä¸­æå–çš„å®ä½“: {question_entities}")
        
        if not question_entities:
            print("âš ï¸ æœªæ‰¾åˆ°å®ä½“ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢")
            return self._vector_search(question, k)
            
        relevant_docs = set()
        graph_scores = defaultdict(float)  # å­˜å‚¨å›¾æ£€ç´¢å¾—åˆ†
        
        for entity in question_entities:
            if entity in self.graph:
                # é™åˆ¶PageRankæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé¿å…æ— å…³æ‰©æ•£
                personalization = {node: 1.0 if node == entity else 0.0 
                                for node in self.graph.nodes()}
                try:
                    ranks = nx.pagerank(self.graph, 
                                      personalization=personalization,
                                      max_iter=self.max_pr_iter)
                    
                    # è·å–æ–‡æ¡£èŠ‚ç‚¹
                    doc_ranks = {node: rank for node, rank in ranks.items() 
                               if node.startswith('doc_')}
                    
                    # ç´¯åŠ æ¯ä¸ªå®ä½“çš„PageRankå¾—åˆ†
                    for doc_id, rank in doc_ranks.items():
                        graph_scores[doc_id] += rank
                    
                    print(f"âœ… å®ä½“ '{entity}' æˆåŠŸæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"âš ï¸ PageRankè®¡ç®—å¤±è´¥: {e}")
                    continue
        
        # å¦‚æœå›¾æ£€ç´¢æ‰¾åˆ°äº†æ–‡æ¡£
        if graph_scores:
            # è·å–åˆæ­¥çš„top-k*2æ–‡æ¡£
            candidate_docs = sorted(graph_scores.items(), 
                                 key=lambda x: x[1], 
                                 reverse=True)[:k*2]
            
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é‡æ’åº
            texts = [self.graph.nodes[doc_id]["text"] for doc_id, _ in candidate_docs]
            if texts:  # ç¡®ä¿æœ‰æ–‡æ¡£å†ç¼–ç 
                doc_vecs = self.embedder.encode(texts, convert_to_numpy=True)
                q_vec = self.embedder.encode([question], convert_to_numpy=True)
                sims = cosine_similarity(q_vec, doc_vecs)[0]
                
                # ç»¼åˆè€ƒè™‘å›¾å¾—åˆ†å’Œå‘é‡ç›¸ä¼¼åº¦
                final_scores = [(doc_id, 0.5 * graph_score + 0.5 * sims[i])
                              for i, (doc_id, graph_score) in enumerate(candidate_docs)]
                
                # å–top-k
                top_docs = sorted(final_scores, key=lambda x: x[1], reverse=True)[:k]
                relevant_docs.update(doc_id for doc_id, _ in top_docs)
                
                print(f"ğŸ“Š å›¾æ£€ç´¢æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # å¦‚æœéœ€è¦è¡¥å……æ–‡æ¡£
        if len(relevant_docs) < k:
            needed = k - len(relevant_docs)
            print(f"âš ï¸ å›¾æ£€ç´¢æ–‡æ¡£ä¸è¶³ï¼Œéœ€è¦è¡¥å…… {needed} ä¸ªæ–‡æ¡£")
            
            # è·å–å‘é‡æ£€ç´¢ç»“æœ
            vector_docs = self._vector_search(question, needed * 2)  # å¤šæ£€ç´¢ä¸€äº›å€™é€‰
            
            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
            vector_vecs = self.embedder.encode(vector_docs, convert_to_numpy=True)
            q_vec = self.embedder.encode([question], convert_to_numpy=True)
            sims = cosine_similarity(q_vec, vector_vecs)[0]
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿‡æ»¤å·²æœ‰æ–‡æ¡£
            scored_docs = [(doc, sim) for doc, sim in zip(vector_docs, sims)
                          if self.doc_map[doc] not in relevant_docs]
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            # æ·»åŠ top-neededæ–‡æ¡£
            fallback_docs = {self.doc_map[doc] for doc, _ in scored_docs[:needed]}
            relevant_docs.update(fallback_docs)
            print(f"ğŸ“Š å‘é‡æ£€ç´¢è¡¥å……äº† {len(fallback_docs)} ä¸ªæ–‡æ¡£")
        
        retrieval_time = time.time() - start_time
        print(f"â±ï¸ æ€»æ£€ç´¢æ—¶é—´: {retrieval_time:.2f}ç§’")
        
        return [self.graph.nodes[doc_id]['text'] 
                for doc_id in list(relevant_docs)[:k]]

    def _vector_search(self, question: str, k: int) -> List[str]:
        """æ”¹è¿›çš„å‘é‡æ£€ç´¢ï¼Œå¢åŠ è°ƒè¯•ä¿¡æ¯"""
        start_time = time.time()
        q_vec = self.embedder.encode([question], convert_to_numpy=True)
        scores = cosine_similarity(q_vec, self.doc_vecs)[0]
        topk_idx = np.argsort(scores)[-k:][::-1]
        topk_scores = scores[topk_idx]
        
        print(f"ğŸ“Š å‘é‡æ£€ç´¢å¾—åˆ†èŒƒå›´: {topk_scores.min():.3f} - {topk_scores.max():.3f}")
        retrieval_time = time.time() - start_time
        print(f"â±ï¸ å‘é‡æ£€ç´¢æ—¶é—´: {retrieval_time:.2f}ç§’")
        
        return [self.docs[i] for i in topk_idx]


class RaptorRAG:
    def __init__(self, 
                 docs: List[str], 
                 embed_model: str = "all-MiniLM-L6-v2", 
                 max_tokens_per_node: int = 1000,
                 use_summary: bool = True,
                 use_summary_for_embedding: bool = True,
                 summary_batch_size: int = 5):
        """åˆå§‹åŒ– RAPTOR æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            embed_model: ç¼–ç æ¨¡å‹åç§°
            max_tokens_per_node: æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§tokenæ•°
            use_summary: æ˜¯å¦ä½¿ç”¨æ‘˜è¦åŠŸèƒ½
            use_summary_for_embedding: æ˜¯å¦ä½¿ç”¨æ‘˜è¦è®¡ç®—embedding
            summary_batch_size: æ¯æ‰¹å¤„ç†çš„æ‘˜è¦æ•°é‡
        """
        self.docs = docs
        self.embedder = SentenceTransformer(embed_model)
        self.max_tokens_per_node = max_tokens_per_node
        self.use_summary = use_summary
        self.use_summary_for_embedding = use_summary_for_embedding
        self.summary_batch_size = summary_batch_size
        self.tree = self._build_tree()
        self.summaries_updated = False
        print(f"âœ… åˆå§‹åŒ– RAPTOR çŸ¥è¯†åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(docs)}")
        print(f"ğŸ“Š é…ç½®ï¼šä½¿ç”¨æ‘˜è¦={use_summary}, æ‘˜è¦embedding={use_summary_for_embedding}, æ‰¹é‡å¤§å°={summary_batch_size}")
        
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        try:
            enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        except KeyError:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
        
    def _chunk_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å—ï¼Œç¡®ä¿æ¯å—ä¸è¶…è¿‡æœ€å¤§tokenæ•°"""
        chunks = []
        current_chunk = []
        current_length = 0
        
        sentences = re.split('([.!?ã€‚ï¼ï¼Ÿ] )', text)
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            if i + 1 < len(sentences):
                sentence += sentences[i + 1]  # æ·»åŠ åˆ†éš”ç¬¦
                
            sentence_length = self._count_tokens(sentence)
            
            if current_length + sentence_length > self.max_tokens_per_node:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
                
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        return chunks

    def _generate_summary(self, texts: List[str], api_key: str = None, model: str = None, base_url: str = None) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        if not texts:
            return ""
            
        combined_text = " ".join(texts)
        # ç®€åŒ–æç¤ºæ¨¡æ¿
        prompt = f"""Please summarize the following text in 3 sentences:

{combined_text}"""
        
        if api_key and model and base_url:
            try:
                print(f"ğŸ”„ æ­£åœ¨ä¸ºé•¿åº¦ä¸º {len(combined_text)} çš„æ–‡æœ¬ç”Ÿæˆæ‘˜è¦...")
                summary = call_openai_chat(prompt, api_key, model, base_url)
                if not summary:
                    print("âš ï¸ API è¿”å›ç©ºæ‘˜è¦ï¼Œä½¿ç”¨æå–å¼æ‘˜è¦")
                    return self._extract_summary(combined_text)
                return summary.strip()
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
                return self._extract_summary(combined_text)
        else:
            return self._extract_summary(combined_text)
            
    def _extract_summary(self, text: str, max_length: int = 200) -> str:
        """æå–å¼æ‘˜è¦æ–¹æ³•ï¼ˆä½œä¸ºç”Ÿæˆå¼æ‘˜è¦çš„é™çº§æ–¹æ¡ˆï¼‰
        
        Args:
            text: æºæ–‡æœ¬
            max_length: æœ€å¤§æ‘˜è¦é•¿åº¦
            
        Returns:
            str: æå–çš„æ‘˜è¦
        """
        # 1. åˆ†å¥
        sentences = re.split('[.!?ã€‚ï¼ï¼Ÿ] ', text)
        if not sentences:
            return text[:max_length] + "..."
            
        # 2. è®¡ç®—æ¯å¥è¯çš„é‡è¦æ€§åˆ†æ•°ï¼ˆè¿™é‡Œç”¨ç®€å•çš„é•¿åº¦å’Œä½ç½®åŠ æƒï¼‰
        scores = []
        for i, sent in enumerate(sentences):
            length_score = min(len(sent.split()) / 20, 1.0)  # é•¿åº¦åˆ†æ•°
            position_score = 1.0 - (i / len(sentences))  # ä½ç½®åˆ†æ•°
            scores.append(length_score * 0.5 + position_score * 0.5)
            
        # 3. é€‰æ‹©æœ€é‡è¦çš„å¥å­
        sorted_sents = [sent for _, sent in sorted(zip(scores, sentences), reverse=True)]
        summary = " ".join(sorted_sents[:3])  # å–top3å¥å­
        
        return summary[:max_length] + ("..." if len(summary) > max_length else "")

    def _build_tree(self) -> Dict:
        """æ„å»ºæ–‡æ¡£æ ‘ç»“æ„"""
        tree = {
            "root": {
                "content": "",
                "summary": "",
                "children": [],
                "embedding": None,
                "id": "root"
            }
        }
        
        # ç¬¬ä¸€å±‚ï¼šå°†æ–‡æ¡£åˆ†ç»„
        for i, doc in enumerate(self.docs):
            chunks = self._chunk_text(doc)
            doc_node = {
                "content": doc,
                "summary": "",
                "children": [],
                "embedding": None,  # å°†åœ¨åç»­æ›´æ–°
                "id": f"doc_{i}"
            }
            
            # ç¬¬äºŒå±‚ï¼šæ–‡æ¡£å—
            for j, chunk in enumerate(chunks):
                chunk_node = {
                    "content": chunk,
                    "summary": "",
                    "children": [],
                    "embedding": self.embedder.encode(chunk, convert_to_numpy=True),
                    "id": f"doc_{i}_chunk_{j}"
                }
                doc_node["children"].append(chunk_node)
            
            # è®¡ç®—æ–‡æ¡£çº§åˆ«çš„embedding
            if self.use_summary and self.use_summary_for_embedding and doc_node["summary"]:
                doc_node["embedding"] = self.embedder.encode(doc_node["summary"], convert_to_numpy=True)
            else:
                doc_node["embedding"] = self.embedder.encode(doc_node["content"], convert_to_numpy=True)
            
            tree["root"]["children"].append(doc_node)
            
        return tree

    def _recursive_search(self, query_vec: np.ndarray, node: Dict, top_k: int, results: List[tuple]) -> None:
        """é€’å½’æœç´¢ç›¸å…³å†…å®¹
        
        Args:
            query_vec: æŸ¥è¯¢å‘é‡
            node: å½“å‰èŠ‚ç‚¹
            top_k: è¿”å›ç»“æœæ•°é‡
            results: ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (node_id, content, score) ä¸‰å…ƒç»„
        """
        # å¦‚æœèŠ‚ç‚¹æœ‰ embeddingï¼Œè®¡ç®—ç›¸ä¼¼åº¦
        if node.get("embedding") is not None:
            score = float(cosine_similarity(query_vec.reshape(1, -1), 
                                         node["embedding"].reshape(1, -1))[0][0])
            results.append((node["id"], node["content"], score))
            
        # é€’å½’æœç´¢å­èŠ‚ç‚¹
        for child in node.get("children", []):
            self._recursive_search(query_vec, child, top_k, results)

    def rag_qa(self, question: str, k: int = 5) -> Dict:
        """ä¸ NaiveRAG å…¼å®¹çš„æ£€ç´¢æ¥å£
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        start_time = time.time()
        
        # ç¼–ç é—®é¢˜
        q_vec = self.embedder.encode([question], convert_to_numpy=True)[0]
        
        # æ”¶é›†æ£€ç´¢ç»“æœ
        results = []
        self._recursive_search(q_vec, self.tree["root"], k, results)
        
        # æ’åºå¹¶è·å– top-k
        results.sort(key=lambda x: x[2], reverse=True)
        top_k_results = results[:k]
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        retrieval_time = time.time() - start_time
        scores = [score for _, _, score in top_k_results]
        
        return {
            "docs": [{"id": doc_id, "text": content} for doc_id, content, _ in top_k_results],
            "doc_scores": scores,
            "metrics": {
                "retrieval_time": retrieval_time,
                "avg_similarity": float(np.mean(scores)),
                "max_similarity": float(np.max(scores)),
                "total_docs_searched": len(self.docs)
            }
        }

    def update_summaries(self, api_key: str, model: str, base_url: str) -> None:
        """æ›´æ–°æ ‘ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„æ‘˜è¦"""
        if not self.use_summary:
            print("âš ï¸ æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ›´æ–°")
            return
            
        def recursive_update(node):
            if node.get("children"):
                # åˆ†æ‰¹å¤„ç†å­èŠ‚ç‚¹
                children = node.get("children", [])
                for i in range(0, len(children), self.summary_batch_size):
                    batch = children[i:i + self.summary_batch_size]
                    print(f"ğŸ“ å¤„ç†ç¬¬ {i//self.summary_batch_size + 1} æ‰¹ï¼Œå…± {len(batch)} ä¸ªèŠ‚ç‚¹")
                    
                    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦
                    for child in batch:
                        if child.get("children"):
                            child_contents = [c["content"] for c in child["children"]]
                            child["summary"] = self._generate_summary(child_contents, api_key, model, base_url)
                            
                            # å¦‚æœé…ç½®ä¸ºä½¿ç”¨æ‘˜è¦è®¡ç®—embeddingï¼Œåˆ™æ›´æ–°embedding
                            if self.use_summary_for_embedding and child["summary"]:
                                child["embedding"] = self.embedder.encode(child["summary"], convert_to_numpy=True)
                                
                            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                            recursive_update(child)
                    
                    # æ¯æ‰¹å¤„ç†å®Œåæš‚åœä¸€ä¸‹
                    if i + self.summary_batch_size < len(children):
                        print("â³ æš‚åœ1ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                        time.sleep(1)
        
        print("ğŸ“ å¼€å§‹æ›´æ–°æ ‘ç»“æ„æ‘˜è¦...")
        recursive_update(self.tree["root"])
        self.summaries_updated = True
        print("âœ… æ‘˜è¦æ›´æ–°å®Œæˆ")


# ä¸»ç¨‹åº

def normalize_answer(s: str) -> str:
    """
    è§„èŒƒåŒ–ç­”æ¡ˆå­—ç¬¦ä¸²ï¼Œç”¨äºæ¯”è¾ƒ
    1. è½¬æ¢ä¸ºå°å†™
    2. ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
    3. ç§»é™¤å† è¯(a, an, the)ç­‰åœç”¨è¯
    """
    import re
    from string import punctuation
    
    # è½¬æ¢ä¸ºå°å†™
    s = s.lower()
    
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    s = s.translate(str.maketrans("", "", punctuation))
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    s = " ".join(s.split())
    
    # ç§»é™¤åœç”¨è¯
    stop_words = {"a", "an", "the", "is", "are", "was", "were"}
    s = " ".join([w for w in s.split() if w not in stop_words])
    
    return s

def compute_exact_match(prediction: str, ground_truth: str) -> float:
    """
    è®¡ç®—Exact Matchåˆ†æ•°
    """
    return float(normalize_answer(prediction) == normalize_answer(ground_truth))

def compute_f1(prediction: str, ground_truth: str) -> float:
    """
    è®¡ç®—F1åˆ†æ•°
    """
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()
    
    # è®¡ç®—é‡å çš„tokenæ•°é‡
    common = set(prediction_tokens) & set(ground_truth_tokens)
    
    if not common:
        return 0.0
    
    precision = len(common) / len(prediction_tokens)
    recall = len(common) / len(ground_truth_tokens)
    
    f1 = 2 * precision * recall / (precision + recall)
    return f1

def evaluate_answer(prediction: str, ground_truth: str) -> dict:
    """
    è¯„ä¼°é¢„æµ‹ç­”æ¡ˆçš„è´¨é‡
    """
    return {
        "exact_match": compute_exact_match(prediction, ground_truth),
        "f1": compute_f1(prediction, ground_truth)
    }

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    try:
        enc = tiktoken.encoding_for_model(model)
    except KeyError:
        enc = tiktoken.get_encoding("cl100k_base")  # fallback
    return len(enc.encode(text))

def calculate_overall_metrics(all_metrics):
    """
    è®¡ç®—æ‰€æœ‰æŸ¥è¯¢çš„å¹³å‡æ€§èƒ½æŒ‡æ ‡
    """
    total_queries = len(all_metrics)
    if total_queries == 0:
        return {}
    
    overall = {
        "avg_exact_match": sum(m["evaluation_metrics"]["exact_match"] for m in all_metrics) / total_queries,
        "avg_f1": sum(m["evaluation_metrics"]["f1"] for m in all_metrics) / total_queries,
        "avg_retrieval_time": sum(m["total_retrieval_time"] for m in all_metrics) / total_queries,
        "avg_docs_searched": sum(m["total_docs_searched"] for m in all_metrics) / total_queries,
        "avg_similarity": sum(m["avg_similarity"] for m in all_metrics) / total_queries,
        "avg_prompt_tokens_per_subquery": sum(m["avg_prompt_tokens"] for m in all_metrics) / total_queries,
        "avg_total_tokens_per_query": sum(m["total_prompt_tokens"] for m in all_metrics) / total_queries
    }
    return overall


def convert_to_query_format(df, dataset_name):
    """å°†ä¸åŒæ•°æ®é›†çš„DataFrameè½¬æ¢ä¸ºç»Ÿä¸€çš„æŸ¥è¯¢æ ¼å¼"""
    if dataset_name in ["hotpot_qa", "trivia_qa", "gsm8k", "sotu_qa"]:
        # è¿™äº›æ•°æ®é›†å·²ç»ä½¿ç”¨ question/answer æ ¼å¼
        return [{"query": row["question"], "ground_truth": row["answer"]} 
                for _, row in df.iterrows()]
    elif dataset_name in ["physics_question", "sports_understanding", "disfl_qa", "strategy_qa"]:
        # è¿™äº›æ•°æ®é›†ä½¿ç”¨ input/target æ ¼å¼
        return [{"query": row["input"], "ground_truth": row["target"]} 
                for _, row in df.iterrows()]
    elif dataset_name == "fever":
        # FEVERæ•°æ®é›†ä½¿ç”¨ claim/label æ ¼å¼
        return [{"query": row["claim"], "ground_truth": row["label"]} 
                for _, row in df.iterrows()]
    else:
        raise ValueError(f"Unsupported dataset format: {dataset_name}")


def get_fused_final_answer(original_question: str, subquery_results: List[Dict], api_key: str, model: str, base_url: str) -> tuple:
    prompt = f"""You are a multi-hop reasoning assistant. Your task is to generate the final answer to a multi-hop question based on the following reasoning steps.

Original Question: {original_question}

Subquestion Reasoning Steps:
"""
    for r in subquery_results:
        prompt += f"{r['subquery_id']}: {r['actual_query']} â†’ {r['answer']}\n"
        prompt += f"Reason: {r['reason']}\n\n"

    prompt += """\nBased on the above reasoning steps, what is the final answer to the original question?

Please respond in JSON format:
{
  "answer": "final_answer",
  "reason": "final_reasoning"
}
Only output valid JSON. Do not add any explanation or markdown code block markers."""

    token_count = count_tokens(prompt, model)
    print(f"ğŸ§  Fusion Prompt Token Count: {token_count}")

    response = call_openai_chat(prompt, api_key, model, base_url)
    try:
        cleaned_response = response.strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()

        parsed = json.loads(cleaned_response)
        answer = parsed.get("answer", "").strip()
        reason = parsed.get("reason", "").strip()
        print(f"âœ… Final fused answer: {answer}")
        print(f"ğŸ” Final reasoning: {reason}")
        return answer, reason, token_count, prompt
    except Exception as e:
        print(f"âš ï¸ Failed to parse fused answer: {e}")
        return "", "", token_count, prompt


def main(decompose: bool = True, use_routing: bool = True, use_reflection: bool = True, max_reflexion_times: int = 2, dataset: str = "hotpot_qa", sample_size: int = 100, openai_model: str = "deepseek-chat", openai_api_key: str = None, openai_base_url: str = None, rag_type: str = "naive"):
    """
    ä¸»å‡½æ•°
    Args:
        decompose: æ˜¯å¦åˆ†è§£æŸ¥è¯¢
        use_routing: æ˜¯å¦ä½¿ç”¨è·¯ç”±
        use_reflection: æ˜¯å¦ä½¿ç”¨åæ€æœºåˆ¶
        max_reflexion_times: æœ€å¤§åæ€æ¬¡æ•°
        dataset: æ•°æ®é›†åç§°
        sample_size: æ ·æœ¬å¤§å°
        openai_model: OpenAIæ¨¡å‹åç§°
        openai_api_key: OpenAI APIå¯†é’¥
        openai_base_url: OpenAI APIåŸºç¡€URL
        rag_type: RAGç±»å‹ï¼Œå¯é€‰å€¼ä¸º"naive"ã€"graph"æˆ–"raptor"
    """
    # å®šä¹‰å¤šä¸ªæŸ¥è¯¢å’Œå¯¹åº”çš„ground truth
    # åŠ è½½æ•°æ®
    with open(f"data/rag/{dataset}.json", "r") as f:
        data = json.load(f)

    queries_and_truth = [
        {
            "query": item["question"],
            "ground_truth": item["answer"]
        }
        for item in data[:sample_size]
    ]

    save_dir = "outputs/"
    save_dir += rag_type
    save_dir += "_"
    save_dir += dataset
    save_dir += "_"
    if not use_routing:
        save_dir += "_no_routing"
    if not decompose:
        save_dir += "_no_decompose"
    if not use_reflection:
        save_dir += "_no_reflection"
    os.makedirs(save_dir, exist_ok=True)

    openai_model = openai_model
    openai_api_key = openai_api_key
    openai_base_url = openai_base_url
    if not openai_api_key:
        raise ValueError("âŒ Please set your OPENAI_API_KEY environment variable.")

    # å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
    with open(f"data/rag/{dataset}_corpus_local.json", "r") as f:
        data = json.load(f)
    local_docs = [f"{item['title']}. {item['text']}" for item in data]
    print(f"âœ… Loaded {len(local_docs)} documents into local_docs.")

    with open(f"data/rag/{dataset}_corpus_global.json", "r") as f:
        data = json.load(f)
    global_docs = [f"{item['title']}. {item['text']}" for item in data]
    print(f"âœ… Loaded {len(global_docs)} documents into global_docs.")

    # è¯»å– profiles.json
    with open(f"data/rag/{dataset}_corpus_profiles.json", "r") as f:
        profiles = json.load(f)
    local_profile = profiles["local_profile"]
    global_profile = profiles["global_profile"]
    print(f"âœ… Loaded local_profile and global_profile.")

    merged_docs = local_docs + global_docs

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    if use_routing:
        if rag_type == "naive":
            local_rag = NaiveRAG(local_docs)
            global_rag = NaiveRAG(global_docs)
        elif rag_type == "graph":
            local_rag = GraphRAG_Improved(local_docs)
            global_rag = GraphRAG_Improved(global_docs)
        elif rag_type == "raptor":
            # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
            use_summary = os.environ.get("RAPTOR_USE_SUMMARY", "true").lower() == "true"
            use_summary_for_embedding = os.environ.get("RAPTOR_USE_SUMMARY_EMBEDDING", "true").lower() == "true"
            summary_batch_size = int(os.environ.get("RAPTOR_SUMMARY_BATCH_SIZE", "1"))
            
            print(f"ğŸ”§ RAPTORé…ç½®ï¼šæ‘˜è¦={use_summary}, æ‘˜è¦å‘é‡={use_summary_for_embedding}, æ‰¹é‡={summary_batch_size}")
            
            local_rag = RaptorRAG(
                docs=local_docs,
                use_summary=use_summary,
                use_summary_for_embedding=use_summary_for_embedding,
                summary_batch_size=summary_batch_size
            )
            global_rag = RaptorRAG(
                docs=global_docs,
                use_summary=use_summary,
                use_summary_for_embedding=use_summary_for_embedding,
                summary_batch_size=summary_batch_size
            )
            
            if use_summary:
                print("æ­£åœ¨ç”Ÿæˆæœ¬åœ°çŸ¥è¯†åº“æ‘˜è¦...")
                local_rag.update_summaries(openai_api_key, openai_model, openai_base_url)
                print("æ­£åœ¨ç”Ÿæˆå…¨å±€çŸ¥è¯†åº“æ‘˜è¦...")
                global_rag.update_summaries(openai_api_key, openai_model, openai_base_url)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ RAG ç±»å‹: {rag_type}")
        print(f"ğŸ” ä½¿ç”¨è·¯ç”±æ¨¡å¼ï¼šåˆ†åˆ«åˆå§‹åŒ–localå’ŒglobalçŸ¥è¯†åº“ï¼ŒRAGç±»å‹ï¼š{rag_type}")
    else:
        # åˆå¹¶æ•°æ®é›†
        if rag_type == "naive":
            merged_rag = NaiveRAG(merged_docs)
        elif rag_type == "graph":
            merged_rag = GraphRAG_Improved(merged_docs)
        elif rag_type == "raptor":
            # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
            use_summary = os.environ.get("RAPTOR_USE_SUMMARY", "false").lower() == "true"
            use_summary_for_embedding = os.environ.get("RAPTOR_USE_SUMMARY_EMBEDDING", "false").lower() == "true"
            summary_batch_size = int(os.environ.get("RAPTOR_SUMMARY_BATCH_SIZE", "1"))
            
            print(f"ğŸ”§ RAPTORé…ç½®ï¼šæ‘˜è¦={use_summary}, æ‘˜è¦å‘é‡={use_summary_for_embedding}, æ‰¹é‡={summary_batch_size}")
            
            merged_rag = RaptorRAG(
                docs=merged_docs,
                use_summary=use_summary,
                use_summary_for_embedding=use_summary_for_embedding,
                summary_batch_size=summary_batch_size
            )
            
            if use_summary:
                print("æ­£åœ¨ç”Ÿæˆåˆå¹¶çŸ¥è¯†åº“æ‘˜è¦...")
                merged_rag.update_summaries(openai_api_key, openai_model, openai_base_url)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ RAG ç±»å‹: {rag_type}")
        print(f"ğŸ” ä½¿ç”¨æ— è·¯ç”±æ¨¡å¼ï¼šåˆå¹¶localå’ŒglobalçŸ¥è¯†åº“ï¼ŒRAGç±»å‹ï¼š{rag_type}")

    all_metrics = []  # å­˜å‚¨æ‰€æœ‰æŸ¥è¯¢çš„æ€§èƒ½æŒ‡æ ‡

    # å¤„ç†æ¯ä¸ªæŸ¥è¯¢
    for idx, query_info in enumerate(queries_and_truth, 1):
        multi_hop_query = query_info["query"]
        ground_truth = query_info["ground_truth"]
        
        print(f"\nğŸ“ å¤„ç†æŸ¥è¯¢ {idx}/{len(queries_and_truth)}:")
        print(f"Query: {multi_hop_query}")
        print(f"Ground Truth: {ground_truth}")
        
        # åˆå§‹åŒ–variable_valueså­—å…¸
        variable_values = {}
        
        if decompose:
            # è·å–å¸¦ä¾èµ–å…³ç³»çš„å­æŸ¥è¯¢è®¡åˆ’
            query_plan = plan_subqueries_with_llm(multi_hop_query, openai_api_key, openai_model, openai_base_url)
            if not query_plan or not query_plan["subqueries"]:
                print("âŒ å­é—®é¢˜è§„åˆ’å¤±è´¥ï¼Œè·³è¿‡å½“å‰æŸ¥è¯¢ã€‚")
                continue
        else:
            # ä¸åˆ†è§£æŸ¥è¯¢ï¼Œç›´æ¥ä½œä¸ºå•ä¸ªé—®é¢˜å¤„ç†
            query_plan = {
                "subqueries": [{
                    "id": "q1",
                    "query": multi_hop_query,
                    "depends_on": [],
                    "variables": []
                }]
            }

        results, fused_answer_texts = [], []
        performance_metrics = {
            "total_retrieval_time": 0,
            "total_docs_searched": 0,
            "avg_similarity_scores": [],
            "max_similarity_scores": [],
            "subquery_metrics": [],
            "total_prompt_tokens": 0,
            "prompt_token_counts": [],
            "evaluation_metrics": {
                "exact_match": 0,
                "f1": 0,
                "final_answer": "",
                "ground_truth": ground_truth
            },
            "evaluation_metrics_fallback": {
                "exact_match": 0,
                "f1": 0,
                "ground_truth": ground_truth
            }
        }

        # æŒ‰é¡ºåºå¤„ç†å­æŸ¥è¯¢ï¼Œå¤„ç†ä¾èµ–å…³ç³»
        for subquery_info in query_plan["subqueries"]:
            subquery_start_time = time.time()
            subquery_id = subquery_info["id"]
            original_query = subquery_info["query"]
            
            # æ£€æŸ¥å¹¶ç­‰å¾…æ‰€æœ‰ä¾èµ–é¡¹å®Œæˆ
            if subquery_info["depends_on"]:
                print(f"\nâ³ å¤„ç†æŸ¥è¯¢ {subquery_id} çš„ä¾èµ–é¡¹: {subquery_info['depends_on']}")
                
            # æ›¿æ¢æŸ¥è¯¢ä¸­çš„å˜é‡
            current_variables = {}
            for var in subquery_info.get("variables", []):
                var_name = var["name"]
                source_query = var["source_query"]
                if source_query not in variable_values:
                    print(f"âŒ é”™è¯¯ï¼šæŸ¥è¯¢ {subquery_id} ä¾èµ–äºæœªå®Œæˆçš„æŸ¥è¯¢ {source_query}")
                    continue
                current_variables[var_name] = variable_values[source_query]
            
            # æ›¿æ¢å˜é‡åçš„å®é™…æŸ¥è¯¢
            actual_query = substitute_variables(original_query, current_variables)
            print(f"\nğŸ” å¤„ç†æŸ¥è¯¢ {subquery_id}: {actual_query}")
            print(f"åŸå§‹æŸ¥è¯¢: {original_query}")
            if current_variables:
                print(f"æ›¿æ¢å˜é‡: {current_variables}")

            # Loop for reflexion
            fail_history = ""
            left_reflexion_times = max_reflexion_times
            while True and left_reflexion_times > 0:
                left_reflexion_times -= 1
                success = 0  # åˆå§‹åŒ–successä¸º0
                if use_routing:
                    route = route_query_with_llm(actual_query, local_profile, global_profile,
                                            api_key=openai_api_key, model=openai_model, base_url=openai_base_url, fail_history=fail_history)
                    rag = local_rag if route == "local" else global_rag
                    print(f"Routing to {route.upper()} DB")
                else:
                    rag = merged_rag
                    route = "merged"
                    print(f"Using merged DB")

                try:
                    retrieved = rag.rag_qa(actual_query, k=5)
                    
                    # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                    metrics = retrieved["metrics"]
                    performance_metrics["total_retrieval_time"] += metrics["retrieval_time"]
                    performance_metrics["total_docs_searched"] += metrics["total_docs_searched"]
                    performance_metrics["avg_similarity_scores"].append(metrics["avg_similarity"])
                    performance_metrics["max_similarity_scores"].append(metrics["max_similarity"])
                    
                    subquery_metrics = {
                        "subquery_id": subquery_id,
                        "retrieval_time": metrics["retrieval_time"],
                        "docs_searched": metrics["total_docs_searched"],
                        "avg_similarity": metrics["avg_similarity"],
                        "max_similarity": metrics["max_similarity"]
                    }
                    performance_metrics["subquery_metrics"].append(subquery_metrics)
                    
                    prompt = f"""Answer the following question based on the provided documents.

    Please respond strictly in JSON format with the following fields:
    - answer: the direct, concise answer (just the value/entity/fact, no explanation). Leave it empty ("") if the answer is not found.
    - reason: a brief explanation of how you arrived at this answer.
    - success: 1 if the answer is confidently found from the documents, 0 otherwise.

    Format:
    {{
        "answer": "...",
        "reason": "...",
        "success": 1
    }}

    If the answer is not mentioned or cannot be inferred from the documents, return:
    {{
        "answer": "",
        "reason": "no relevant information found",
        "success": 0
    }}

    Question: {actual_query}

    Documents:
    """
                    for d in retrieved["docs"]:
                        prompt += f"- {d}\n"
                    prompt += "\nOnly output valid JSON. Do not add any explanation or markdown code block markers."

                    token_count = count_tokens(prompt, openai_model)
                    print(f"ğŸ§® Prompt Token Count: {token_count}")
                    
                    response = call_openai_chat(prompt, openai_api_key, openai_model, openai_base_url)
                    try:
                        # æ¸…ç†å“åº”ä¸­å¯èƒ½å­˜åœ¨çš„markdownä»£ç å—æ ‡è®°
                        cleaned_response = response.strip()
                        if cleaned_response.startswith("```json"):
                            cleaned_response = cleaned_response[7:]
                        if cleaned_response.endswith("```"):
                            cleaned_response = cleaned_response[:-3]
                        cleaned_response = cleaned_response.strip()
                        
                        parsed_response = json.loads(cleaned_response)
                        answer = parsed_response["answer"].strip()
                        reason = parsed_response["reason"].strip()
                        success = int(parsed_response["success"])
                        
                        # å­˜å‚¨ç­”æ¡ˆä¾›åç»­æŸ¥è¯¢ä½¿ç”¨
                        if success == 1:
                            variable_values[subquery_id] = answer
                            print(f"æå–çš„ç­”æ¡ˆ: {answer}")
                            print(f"æ¨ç†è¿‡ç¨‹: {reason}")
                            print(f"æ˜¯å¦æˆåŠŸ: {success}")
                        else:
                            variable_values[subquery_id] = ""
                            fail_history += f"Fail History: Last routing failed because {reason}. Last routing result is {route}. So please try another routing choice, don't choose {route} again."
                        
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"âš ï¸ è§£æç­”æ¡ˆå¤±è´¥: {str(e)}")
                        print(f"åŸå§‹å“åº”: {response}")
                        answer, reason = f"Error: {str(e)}", ""
                        success = 0
                    
                    # æ›´æ–°tokenç»Ÿè®¡
                    performance_metrics["total_prompt_tokens"] += token_count
                    performance_metrics["prompt_token_counts"].append(token_count)
                    
                    results.append({
                        "subquery_id": subquery_id,
                        "original_query": original_query,
                        "actual_query": actual_query,
                        "routing": route,
                        "answer": answer,
                        "reason": reason,
                        "docs": retrieved["docs"],
                        "doc_scores": retrieved["doc_scores"],
                        "variables_used": current_variables,
                        "metrics": subquery_metrics,
                        "prompt_token_count": token_count
                    })
                    fused_answer_texts.append(f"{subquery_id}: {actual_query} â†’ {answer} (reason: {reason})")
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    answer, retrieved = f"Error: {str(e)}", {"docs": [], "doc_scores": []}
                    success = 0  # è®¾ç½®successä¸º0

                if success == 1 or use_routing == False or use_reflection == False or left_reflexion_times <= 0:
                    break   # å¦‚æœæˆåŠŸæˆ–è€…ä¸ä½¿ç”¨è·¯ç”±æˆ–è€…ä¸ä½¿ç”¨åå°„æˆ–è€…åå°„æ¬¡æ•°ç”¨å®Œï¼Œåˆ™è·³å‡ºå¾ªç¯

        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        performance_metrics["avg_retrieval_time"] = performance_metrics["total_retrieval_time"] / len(query_plan["subqueries"])
        performance_metrics["avg_similarity"] = np.mean(performance_metrics["avg_similarity_scores"])
        performance_metrics["max_similarity"] = np.max(performance_metrics["max_similarity_scores"])
        
        # è®¡ç®—tokenç»Ÿè®¡æŒ‡æ ‡
        performance_metrics["avg_prompt_tokens"] = performance_metrics["total_prompt_tokens"] / len(query_plan["subqueries"])
        performance_metrics["max_prompt_tokens"] = max(performance_metrics["prompt_token_counts"], default=0)
        performance_metrics["min_prompt_tokens"] = min(performance_metrics["prompt_token_counts"], default=0)

        # è·å– fallbackï¼ˆæœ€åä¸€è·³ï¼‰çš„ç­”æ¡ˆï¼ˆç”¨äºå¯¹æ¯”åˆ†æï¼‰
        fallback_answer = results[-1]["answer"] if results else ""
        performance_metrics["evaluation_metrics"]["fallback_answer"] = fallback_answer


        # è·å–æœ€ç»ˆç­”æ¡ˆï¼ˆèåˆæ‰€æœ‰ reasoning æ­¥éª¤ï¼‰
        final_answer, final_reason, fusion_token_count, fusion_prompt = get_fused_final_answer(
            multi_hop_query, results,
            api_key=openai_api_key,
            model=openai_model,
            base_url=openai_base_url
        )
        performance_metrics["evaluation_metrics"]["final_answer"] = final_answer
        performance_metrics["evaluation_metrics"]["final_reason"] = final_reason
        performance_metrics["fusion_prompt_tokens"] = fusion_token_count

        performance_metrics["total_prompt_tokens"] += fusion_token_count
        performance_metrics["prompt_token_counts"].append(fusion_token_count)

        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        eval_results = evaluate_answer(final_answer, ground_truth)
        eval_results_fallback = evaluate_answer(fallback_answer, ground_truth)
        performance_metrics["evaluation_metrics"].update(eval_results)
        performance_metrics["evaluation_metrics_fallback"].update(eval_results_fallback)
        
        # ä¿å­˜å½“å‰æŸ¥è¯¢çš„ç»“æœ
        query_results_path = os.path.join(save_dir, f"query_{idx}_results.jsonl")
        with open(query_results_path, "w") as f:
            # ç¬¬1æ¡ï¼šQuery metadata
            f.write(json.dumps({
                "type": "query_info",
                "query": multi_hop_query,
                "ground_truth": ground_truth
            }) + "\n")

            # ç¬¬2æ¡ï¼šFinal answer summary
            f.write(json.dumps({
                "type": "final_answer",
                "final_answer": final_answer,
                "final_reason": final_reason,
                "fusion_prompt_tokens": fusion_token_count,
                "fallback_answer": fallback_answer,
                "fusion_equals_fallback": fallback_answer.strip().lower() == final_answer.strip().lower()
            }) + "\n")

            # ç¬¬3æ¡ï¼šEvaluation metrics
            f.write(json.dumps({
                "type": "evaluation_metrics",
                "fusion": {
                    "exact_match": eval_results["exact_match"],
                    "f1": eval_results["f1"]
                },
                "fallback": {
                    "exact_match": eval_results_fallback["exact_match"],
                    "f1": eval_results_fallback["f1"]
                }
            }) + "\n")

            # ç¬¬4æ¡ï¼šPerformance metrics
            f.write(json.dumps({
                "type": "performance_metrics",
                "total_retrieval_time": performance_metrics["total_retrieval_time"],
                "avg_retrieval_time": performance_metrics["avg_retrieval_time"],
                "total_docs_searched": performance_metrics["total_docs_searched"],
                "avg_similarity": performance_metrics["avg_similarity"],
                "max_similarity": performance_metrics["max_similarity"],
                "token_cost": {
                    "total_prompt_tokens": performance_metrics["total_prompt_tokens"],
                    "avg_prompt_tokens": performance_metrics["avg_prompt_tokens"],
                    "max_prompt_tokens": performance_metrics["max_prompt_tokens"],
                    "min_prompt_tokens": performance_metrics["min_prompt_tokens"]
                }
            }) + "\n")

            # ç¬¬5æ¡ï¼šSubquery performance details
            for metrics in performance_metrics["subquery_metrics"]:
                f.write(json.dumps({
                    "type": "subquery_metric",
                    "subquery_id": metrics["subquery_id"],
                    "retrieval_time": metrics["retrieval_time"],
                    "docs_searched": metrics["docs_searched"],
                    "avg_similarity": metrics["avg_similarity"],
                    "max_similarity": metrics["max_similarity"]
                }) + "\n")

            # ç¬¬6æ¡ï¼šExecution results (per subquery)
            for r in results:
                f.write(json.dumps({
                    "type": "execution_result",
                    "subquery_id": r["subquery_id"],
                    "original_query": r["original_query"],
                    "actual_query": r["actual_query"],
                    "variables_used": r.get("variables_used", None),
                    "routing": r["routing"],
                    "answer": r["answer"],
                    "reason": r["reason"],
                    "docs": [
                        {
                            "text": doc,
                            "score": r["doc_scores"][i]
                        }
                        for i, doc in enumerate(r["docs"])
                    ]
                }) + "\n")

            # ç¬¬7æ¡ï¼šAnswer chain
            for step in fused_answer_texts:
                f.write(json.dumps({
                    "type": "fused_answer_step",
                    "text": step
                }) + "\n")        
        # ä¿å­˜èåˆæç¤º
        fusion_prompt_path = os.path.join(save_dir, f"query_{idx}_fusion_prompt.txt")
        with open(fusion_prompt_path, "w") as f_prompt:
            f_prompt.write(fusion_prompt)


        all_metrics.append(performance_metrics)

    # è®¡ç®—å¹¶ä¿å­˜æ•´ä½“æ€§èƒ½æŒ‡æ ‡
    overall_metrics = calculate_overall_metrics(all_metrics)
    
    # ä¿å­˜æ•´ä½“ç»“æœ
    overall_txt_path = os.path.join(save_dir, "overall_results.txt")
    with open(overall_txt_path, "w") as f:
        f.write("ğŸ“Š Overall Performance Summary:\n")
        f.write(f"- Average Exact Match: {overall_metrics['avg_exact_match']:.4f}\n")
        f.write(f"- Average F1 Score: {overall_metrics['avg_f1']:.4f}\n")
        f.write(f"- Average Retrieval Time: {overall_metrics['avg_retrieval_time']:.4f}s\n")
        f.write(f"- Average Documents Searched: {overall_metrics['avg_docs_searched']:.1f}\n")
        f.write(f"- Average Similarity Score: {overall_metrics['avg_similarity']:.4f}\n")
        f.write(f"- Average Prompt Tokens per Subquery: {overall_metrics['avg_prompt_tokens_per_subquery']:.2f}\n")
        f.write(f"- Average Total Tokens per Query: {overall_metrics['avg_total_tokens_per_query']:.2f}\n")

        

    overall_json_path = os.path.join(save_dir, "overall_results.json")
    with open(overall_json_path, "w") as f:
        json.dump({
            "queries": queries_and_truth,
            "overall_metrics": overall_metrics,
            "all_query_metrics": all_metrics
        }, f, indent=2)

    # æ›´æ–°æ§åˆ¶å°è¾“å‡º
    print("\nğŸ“Š Overall Performance Summary:")
    print(f"- Average Exact Match: {overall_metrics['avg_exact_match']:.4f}")
    print(f"- Average F1 Score: {overall_metrics['avg_f1']:.4f}")
    print(f"- Average Retrieval Time: {overall_metrics['avg_retrieval_time']:.4f}s")
    print(f"- Average Documents Searched: {overall_metrics['avg_docs_searched']:.1f}")
    print(f"- Average Similarity Score: {overall_metrics['avg_similarity']:.4f}")
    print(f"- Average Prompt Tokens per Subquery: {overall_metrics['avg_prompt_tokens_per_subquery']:.2f}")
    print(f"- Average Total Tokens per Query: {overall_metrics['avg_total_tokens_per_query']:.2f}")

    print("\nâœ… Results saved to:")
    print(f"   - {overall_txt_path}")
    print(f"   - {overall_json_path}")
    for i in range(len(queries_and_truth)):
        print(f"   - {os.path.join(save_dir, f'query_{i+1}_results.txt')}")


if __name__ == "__main__":
    # é»˜è®¤ä½¿ç”¨åˆ†è§£å’Œè·¯ç”±æ¨¡å¼
    main(decompose=True, 
         use_routing=True, 
         use_reflection=True, 
         max_reflexion_times=2, 
         dataset="hotpot_qa", 
         sample_size=5, 
         openai_model=os.environ.get("OPENAI_MODEL"),  
         openai_api_key=os.environ.get("OPENAI_API_KEY"), 
         openai_base_url=os.environ.get("OPENAI_API_BASE"),
         rag_type=os.environ.get("RAG_TYPE", "naive"))  # naive or others